# ETL Sales Data Pipeline ğŸš€

## ğŸ“Œ Project Overview
This project automates an ETL pipeline that extracts customer sales data from an API and an SQL database, transforms it, and loads it into a PostgreSQL data warehouse. The process is scheduled and monitored using Apache Airflow.

## ğŸ“Œ Features
âœ… Extracts data from an API and a PostgreSQL database  
âœ… Cleans & transforms data (handling nulls, duplicates, aggregations)  
âœ… Loads processed data into a PostgreSQL database  
âœ… Automates the ETL process using Apache Airflow  
âœ… Implements unit testing for validation  
âœ… Deployable via Docker for scalability  

---

## ğŸ“Œ Project Structure
```
etl-sales-pipeline/
â”‚â”€â”€ data/                          # Sample or raw data (ignored in production)
â”‚   â”œâ”€â”€ sample_api_data.json
â”‚   â”œâ”€â”€ sample_sql_data.csv
â”‚
â”‚â”€â”€ dags/                          # Airflow DAGs for scheduling the ETL pipeline
â”‚   â”œâ”€â”€ etl_pipeline.py            # Main DAG definition
â”‚
â”‚â”€â”€ scripts/                       # Python scripts for each ETL step
â”‚   â”œâ”€â”€ extract.py                 # Extract data from API & SQL
â”‚   â”œâ”€â”€ transform.py               # Data cleaning & transformation
â”‚   â”œâ”€â”€ load.py                    # Load transformed data into DB
â”‚
â”‚â”€â”€ config/                        # Configuration files for database & API access
â”‚   â”œâ”€â”€ db_config.yaml             # Database connection settings
â”‚   â”œâ”€â”€ api_config.yaml            # API keys & endpoints
â”‚
â”‚â”€â”€ notebooks/                     # Jupyter notebooks for testing
â”‚   â”œâ”€â”€ etl_pipeline_test.ipynb    # Interactive notebook for debugging ETL
â”‚
â”‚â”€â”€ tests/                         # Unit tests for ETL scripts
â”‚   â”œâ”€â”€ test_extract.py            # Test extraction functions
â”‚   â”œâ”€â”€ test_transform.py          # Test transformation functions
â”‚   â”œâ”€â”€ test_load.py               # Test data loading functions
â”‚
â”‚â”€â”€ logs/                          # Logs directory (ignored in .gitignore)
â”‚
â”‚â”€â”€ .gitignore                      # Ignore unnecessary files (logs, env, data)
â”‚â”€â”€ requirements.txt                # Required Python libraries
â”‚â”€â”€ README.md                       # Project documentation
â”‚â”€â”€ docker-compose.yml              # Docker setup (optional)
â”‚â”€â”€ airflow.cfg                      # Airflow configuration (if using Airflow)
```

---

## ğŸ“Œ Installation & Setup

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/yourusername/etl-sales-pipeline.git
cd etl-sales-pipeline
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Set Up PostgreSQL Database**
Create a PostgreSQL database and configure `config/db_config.yaml` with your credentials.

### **4ï¸âƒ£ Run ETL Pipeline**
Execute each script sequentially:
```bash
python scripts/extract.py
python scripts/transform.py
python scripts/load.py
```

### **5ï¸âƒ£ (Optional) Set Up Airflow for Automation**
```bash
airflow db init
airflow scheduler &
airflow webserver &
```

---

## ğŸ“Œ Technologies Used
- **Python** (ETL scripting)
- **Pandas & SQLAlchemy** (Data processing)
- **PostgreSQL** (Data warehouse)
- **Apache Airflow** (Automation)
- **Docker** (Deployment)

---

## ğŸ“Œ License
This project is licensed under the MIT License.

---

### **ğŸš€ Future Enhancements**
âœ… Implement logging & monitoring using Prometheus & Grafana  
âœ… Improve error handling & exception logging  
âœ… Deploy the pipeline to cloud platforms (AWS, GCP)  

Would you like a sample dataset for testing? ğŸ˜Š

